{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOk5ZDZ2Bpe/uVMAYAtzMOq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AEngly/DeepLearning02456/blob/main/Exercise1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Homework 1**\n",
        "\n",
        "Hand-in: 05/09-2022"
      ],
      "metadata": {
        "id": "talZA5K34ttn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise a)**\n",
        "\n",
        "*Write $y_{j}$ directly as a function of x. That is, eliminate the as and zs:*\n",
        "\n",
        "By insertion and expansion the j'th output $y_{j}$ is\n",
        "\n",
        "\\begin{align}\n",
        "y_{j} &= h_{2}(a_{j}^{(2)}) \\\\\n",
        "      &= h_{2}(\\sum_{i = 0}^{M} w_{ji}^{(2)} z_{i}^{(1)}) \\\\\n",
        "      &= h_{2}(\\sum_{i = 0}^{M} w_{ji}^{(2)} h_{1}(a_{i}^{(1)})) \\\\\n",
        "      &= h_{2}(\\sum_{i = 0}^{M} w_{ji}^{(2)} h_{1}(\\sum_{d = 0}^{D} w_{id}^{(1)} x_{d})) \\\\\n",
        "\\end{align}\n",
        "\n",
        "where the superscripts (1) and (2) denote the weight matrices applied to respectively the input layer and the hidden layer.\n",
        "\n",
        "The matrix form is\n",
        "\n",
        "$$ \\textbf{y} = h_{2}(\\Theta^{(2)} h_{1}(\\Theta^{(1)} \\textbf{x})) $$\n",
        "\n",
        "where $\\Theta$ is a weight matrix corresponding to the (i'th) layer.\n",
        "\n",
        "**Exercise b)**\n",
        "\n",
        "*Write the equation for a neural network with two hidden layers and three layers of weights \n",
        "$w = [w_{1}, w_{2}, w_{3}]$. Again, without using as and zs.* \\\\\n",
        "\n",
        "We extend the result from the previous exercise. Let $M_{1}$ and $M_{2}$ denote the number of units in respectively the first and second layer.\n",
        "\n",
        "\\begin{align}\n",
        "y_{j} &= h_{3}(a_{j}^{(3)}) \\\\\n",
        "      &= h_{3}(\\sum_{i = 0}^{M_{2}} w_{ji}^{(3)} z_{i}^{(2)}) \\\\\n",
        "      &= h_{3}(\\sum_{i = 0}^{M_{2}} w_{ji}^{(3)} h_{2}(a_{i}^{(2)})) \\\\\n",
        "      &= h_{3}(\\sum_{i = 0}^{M_{2}} w_{ji}^{(3)} h_{2}(\\sum_{k = 0}^{M_{1}} w_{ik}^{(2)} z_{k}^{(1)})) k \\\\\n",
        "      &= h_{3}(\\sum_{i = 0}^{M_{2}} w_{ji}^{(3)} h_{2}(\\sum_{k = 0}^{M_{1}} w_{ik}^{(2)} h_{1}(a_{k}^{(1)}))) \\\\\n",
        "      &= h_{3}(\\sum_{i = 0}^{M_{2}} w_{ji}^{(3)} h_{2}(\\sum_{k = 0}^{M_{1}} w_{ik}^{(2)} h_{1}(\\sum_{d = 0}^{D} w_{kd}^{(1)} x_{d}))) \\\\\n",
        "\\end{align}\n",
        "\n",
        "where the superscripts (1), (2), and (3) denote the weight matrices applied to respectively the input layer and the hidden layer. The matrix notation is then\n",
        "\n",
        "$$ \\textbf{y} = h_{3}(\\Theta^{(3)}h_{2}(\\Theta^{(2)} h_{1}(\\Theta^{(1)} \\textbf{x}))) $$\n",
        "\n",
        "where $\\Theta$ is a weight matrix corresponding to the (i'th) layer.\n",
        "\n",
        "**Exercise c)**\n",
        "\n",
        "*Write the equations for an FFNN with L layers as recursion. Use $\\ell$ as the index for the layer:*\n",
        "\n",
        "\\begin{align}\n",
        "y_{j} &= h_{L}(a_{j}^{L}) \\\\\n",
        "z_{j}^{(\\ell)} &= h_{\\ell}(a_{j}^{\\ell}) \\ \\ \\text{for} \\ \\ \\ell = 1, ... , L \\\\\n",
        "a_{j}^{(\\ell)} &= \\sum_{i = 0}^{M_{\\ell - 1}} w_{ji}^{(\\ell)} z_{i}^{(\\ell - 1)}  \\ \\ \\text{for} \\ \\ \\ell = 2, ... , L \\\\\n",
        "a_{j}^{(1)} &= \\sum_{i = 0}^{D} w_{ji}^{(1)} x_{i} \\\\\n",
        "\\end{align}\n",
        "\n",
        "**Exercise e)**\n",
        "\n",
        "*Show that the loss we get is*\n",
        "\n",
        "$$\\frac{ND}{2} \\log 2 \\pi \\sigma^{2} + \\frac{1}{2 \\sigma^{2}} E(w) $$\n",
        "\n",
        "*where*\n",
        "\n",
        "$$E(w) = \\sum_{n=1}^{N} ||\\textbf{y}(\\textbf{x}_{n}) - \\textbf{t}_{n} ||_{2}^{2}$$\n",
        "\n",
        "The general form of the multivariate normal distribution is \n",
        "\n",
        "$$N(\\mathbf{t}_{n} | \\mathbf{y}(\\mathbf{x}_{n}), \\mathbf{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^{D} |\\mathbf{\\Sigma}|}} \\exp\\{-\\frac{1}{2} (\\mathbf{t}_{n} - \\mathbf{y}(\\mathbf{x}_{n}))^{T} \\mathbf{\\Sigma}^{-1} (\\mathbf{t}_{n} - \\mathbf{y}(\\mathbf{x}_{n}))\\}$$\n",
        "\n",
        "Assuming $\\mathbf{\\Sigma} = \\sigma^{2} \\mathbf{I}$ (i.e. indepedent covariates), we can simplify the expression.\n",
        "\n",
        "\\begin{align}\n",
        "N(\\mathbf{t}_{n} | \\mathbf{y}(\\mathbf{x}_{n}), \\sigma^{2} \\mathbf{I}) &= \\frac{1}{\\sqrt{(2\\pi)^{D} \\sigma^{2D}}} \\exp\\{-\\frac{1}{2 \\sigma^{2}} (\\mathbf{t}_{n} - \\mathbf{y}(\\mathbf{x}_{n}))^{T} (\\mathbf{t}_{n} - \\mathbf{y}(\\mathbf{x}_{n}))\\} \\\\\n",
        "\\end{align}\n",
        "\n",
        "Then we construct the likelihood function. As the samples are independent, it is found by multiplication of the individual likelihood contributions.\n",
        "\n",
        "\\begin{align}\n",
        "L(w) &= \\prod_{n=1}^{N} \\frac{1}{\\sqrt{(2\\pi)^{D} \\sigma^{2D}}} \\exp\\{-\\frac{1}{2 \\sigma^{2}} (\\mathbf{t}_{n} - \\mathbf{y}(\\mathbf{x}_{n}))^{T} (\\mathbf{t}_{n} - \\mathbf{y}(\\mathbf{x}_{n}))\\} \\\\\n",
        "&= \\frac{1}{(\\sqrt{(2\\pi)^{D} \\sigma^{2D}})^{N}} \\exp\\{-\\frac{1}{2 \\sigma^{2}} \\sum_{n=1}^{N}(\\mathbf{t}_{n} - \\mathbf{y}(\\mathbf{x}_{n}))^{T} (\\mathbf{t}_{n} - \\mathbf{y}(\\mathbf{x}_{n}))\\} \\\\\n",
        "&= \\frac{1}{(\\sqrt{(2\\pi)^{D} \\sigma^{2D}})^{N}} \\exp\\{-\\frac{1}{2 \\sigma^{2}} \\sum_{n=1}^{N} ||\\textbf{y}(\\textbf{x}_{n}) - \\textbf{t}_{n} ||_{2}^{2}\\} \\\\\n",
        "&= ((2\\pi)^{D} \\sigma^{2D})^{-\\frac{N}{2}} \\exp\\{-\\frac{1}{2 \\sigma^{2}} E(w)\\} \\\\\n",
        "&= ((2\\pi)^{ND} \\sigma^{2ND})^{-\\frac{1}{2}} \\exp\\{-\\frac{1}{2 \\sigma^{2}} E(w)\\} \\\\\n",
        "&= (2\\pi)^{-\\frac{ND}{2}} \\sigma^{-2\\frac{ND}{2}} \\exp\\{-\\frac{1}{2 \\sigma^{2}} E(w)\\} \\\\\n",
        "&= (2\\pi \\sigma^{2})^{-\\frac{ND}{2}}\\exp\\{-\\frac{1}{2 \\sigma^{2}} E(w)\\}\n",
        "\\end{align}\n",
        "\n",
        "Then we apply the natural logarithm because all products will turn into sums. As the differential operator is linear, it makes it easy to find maximum likelihood estimates (MLE's). \n",
        "\n",
        "\\begin{align}\n",
        "\\ln L(w) &= \\ln ((2\\pi \\sigma^{2})^{-\\frac{ND}{2}}\\exp\\{-\\frac{1}{2 \\sigma^{2}} E(w)\\}) \\\\\n",
        "&= -\\frac{ND}{2} \\ln(2\\pi \\sigma^{2}) - \\frac{1}{2 \\sigma^{2}} E(w) \\exp\\{1\\} \\\\\n",
        "&= -\\frac{ND}{2} \\ln(2\\pi \\sigma^{2}) - \\frac{1}{2 \\sigma^{2}} E(w)\n",
        "\\end{align}\n",
        "\n",
        "Multiplying the log-likelihood by -1 yields the desired equation. Hence,\n",
        "\n",
        "\\begin{align}\n",
        "-\\ln L(w) &= \\frac{ND}{2} \\ln(2\\pi \\sigma^{2}) + \\frac{1}{2 \\sigma^{2}} E(w)\n",
        "\\end{align}\n",
        "\n",
        "By changing the sign, we can minimize the negative log-likelihood. This is equivalent to maximizing the positive log-likelihood. However, much software is optimized to solve minimization problems. For this reason, minimization is often preferred. \n",
        "\n",
        "The optimum of the log-likehood is the same as the optimum of the likehood. The reason is that logarithm is a monotonic transformation. Hence, if a given set of weights is the optimum in the log-likelihood, then it follows that it is also the optimum in the normal likelihood function.\n",
        "\n",
        "**Exercise g)**\n",
        "\n",
        "*Show using the same procedure we used for regression that the loss function for classification is:*\n",
        "\n",
        "$$E(w) = -\\sum_{n=1}^{N}\\sum_{k}^{K} t_{nk} \\log y_{k}(\\textbf{x}_{n})$$\n",
        "\n",
        "*This is also known as the cross entropy loss.*\n",
        "\n",
        "Under the assumption of independent sampling the likelihood function becomes\n",
        "\n",
        "\\begin{align}\n",
        "L(\\mathbf{T} | \\mathbf{X}, \\mathbf{w}) &= p(\\mathbf{t}_{1}, ..., \\mathbf{t}_{N} | \\mathbf{x}_{1}, ..., \\mathbf{x}_{n}) \\\\\n",
        "&= \\prod_{n}^{N} p(\\mathbf{t}_{n = 1} | \\mathbf{x}_{n}, \\mathbf{w}) \\\\\n",
        "&= \\prod_{n}^{N} \\prod_{k = 1}^{K} [y_{k}(\\mathbf{x}_{n})]^{t_{nk}}\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbf{T}$ is the matrix of N rows and K columns, and $\\mathbf{X}$ is the input matrix consisting of N rows.\n",
        "\n",
        "This corresponds exactly to finding the joint probability of observing a training set of the given classes as $[y_{k}(\\mathbf{x}_{n})]^{0} = 1$ if $t_{nk} = 0$. Hence, all the correct elements are selected.\n",
        "\n",
        "\\begin{align}\n",
        "E(w) &= -\\log L(\\mathbf{T} | \\mathbf{X}, \\mathbf{w}) \\\\\n",
        "&= -\\log \\prod_{n}^{N} \\prod_{k = 1}^{K} [y_{k}(\\mathbf{x}_{n})]^{t_{nk}} \\\\\n",
        "&= -\\sum_{n = 1}^{N} \\sum_{k = 1}^{K} t_{nk} \\log (y_{k}(\\mathbf{x}_{n}))\n",
        "\\end{align}\n",
        "\n",
        "**Exercise i)**\n",
        "\n",
        "We wish to calculate \n",
        "\n",
        "$$ \\delta_{j}^{(L)} = \\frac{\\partial E(w)}{\\partial a_{j}^{(L)}} $$ \n",
        "\n",
        "The loss function is written in terms of the logits $a_{j}^{(L)}$. \n",
        "\n",
        "\\begin{align}\n",
        "E(w) &= -\\sum_{k = 1}^{K} t_{k} \\log (y_{k}) \\\\\n",
        "&= -\\sum_{k = 1}^{K} t_{k} \\log (\\frac{a_{k}^{(L)}}{\\sum_{j=1}^{K} a_{j}^{(L)}}) \\\\\n",
        "&= -\\sum_{k = 1}^{K} t_{k} \\log (\\frac{\\exp{a_{k}^{(L)}}}{\\sum_{j=1}^{K} \\exp{a_{j}^{(L)}}})  \\\\\n",
        "&= -\\sum_{k = 1}^{K} t_{k} (\\log (\\exp{a_{k}^{(L)}}) -  \\log (\\sum_{j=1}^{K} \\exp{a_{j}^{(L)}})) \\\\\n",
        "&= -\\sum_{k = 1}^{K} t_{k} a_{k}^{(L)} - t_{k} \\log (\\sum_{j=1}^{K} \\exp{a_{j}^{(L)}}) \\\\\n",
        "&= -(\\sum_{k = 1}^{K} t_{k} a_{k}^{(L)}) + t_{k} \\log (\\sum_{j=1}^{K} \\exp{a_{j}^{(L)}}) \\\\\n",
        "\\end{align}\n",
        "\n",
        "As $t_{k}$ is only 1 for 1 of the K class outputs, we can reduce it to\n",
        "\n",
        "\\begin{align}\n",
        "E(w) = -(\\sum_{k = 1}^{K} t_{k} a_{k}^{(L)}) + \\log (\\sum_{j=1}^{K} \\exp{a_{j}^{(L)}}) \\\\\n",
        "\\end{align}\n",
        "\n",
        "which only applies when we have 1 input vector.\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial E(w)}{\\partial a_{j}^{(L)}} &= \\frac{∂}{\\partial a_{j}^{(L)}} (-(\\sum_{k = 1}^{K} t_{k} a_{k}^{(L)}) + \\log (\\sum_{j=1}^{K} \\exp{a_{j}^{(L)}})) \\\\\n",
        "&= -\\frac{∂}{\\partial a_{j}^{(L)}} (\\sum_{k = 1}^{K} t_{k} a_{k}^{(L)}) + \\frac{∂}{\\partial a_{j}^{(L)}} \\log (\\sum_{j=1}^{K} \\exp{a_{j}^{(L)}}) \\\\\n",
        "&= -t_{j} + \\frac{1}{\\exp{ \\sum_{j=1}^{K} a_{k}^{(L)}}} \\frac{∂}{\\partial a_{j}^{(L)}} \\exp{ \\sum_{j=1}^{K} a_{k}^{(L)}} \\\\\n",
        "&= -t_{j} + \\frac{\\exp{a_{j}^{(L)}}}{\\exp{ \\sum_{j=1}^{K} a_{k}^{(L)}}} \\\\\n",
        "&= -t_{j} + y_{k}\n",
        "\\end{align}\n",
        "\n",
        "Hence,\n",
        "\n",
        "$$\\delta_{j}^{(L)} = \\frac{\\partial E(w)}{\\partial a_{j}^{(L)}} = -t_{j} + y_{k}$$\n",
        "\n",
        "\n",
        "**Exercise j)**\n",
        "\n",
        "The formula is written as a recursion to utilize the fact that the results are reused. This means that we can use a top-down approach in dynamical programming to reduce computation time considerably. \n",
        "\n",
        "**Exercise l)**\n",
        "\n",
        "We can reintroduce the summation over n. All $a$'s must be indexed by n as $a_{n,j}^{(1)} = \\sum_{d=0}^{D} w_{ji}^{(1)} x_{n,d}$. As the differential operator is linear the gradient must be the sum of the individual sample contributions. \n",
        "\n",
        "The general backpropagation algorithm is then\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial E(w)}{\\partial w_{ji}^{(l)}} &= \\sum_{n=1}^{N}\\delta_{n,j}^{(l)} z_{n,i}^{(l-1)} \\\\\n",
        "\\delta_{n,j}^{(l)} &= \\sum_{k=1}^{K} \\delta_{n,k}^{(l+1)} w_{ji}^{(l+1)} h_{l}'(a_{n,j}^{(l)}) \\\\\n",
        "&= \\sum_{k=1}^{K} \\delta_{n,k}^{(l+1)} w_{ji}^{(l+1)} h_{l}'(\\sum_{i = 1}^{M_{(l-1)}}w_{ji}^{(l)} z_{n,i}^{(l-1)})\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "Jinx_LhVCIcO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJTnF7sGER3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}